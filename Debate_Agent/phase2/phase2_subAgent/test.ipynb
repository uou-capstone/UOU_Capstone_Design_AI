{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b26d28ca",
   "metadata": {},
   "source": [
    "# 1) intro 생성 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21ddaea0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ClientError",
     "evalue": "403 PERMISSION_DENIED. {'error': {'code': 403, 'message': 'Your API key was reported as leaked. Please use another API key.', 'status': 'PERMISSION_DENIED'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mClientError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     32\u001b[39m dialogue_style = sample_contents[\u001b[33m\"\u001b[39m\u001b[33mmode_configuration\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mdialogue_style\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     33\u001b[39m rules = sample_contents[\u001b[33m\"\u001b[39m\u001b[33msession_rules\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m response = \u001b[43mllm_moderator_generate_intro\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopic_keyword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrules\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopic_description\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdialogue_style\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Debate_Agent/phase2/phase2_subAgent/Gen_intro.py:27\u001b[39m, in \u001b[36mllm_moderator_generate_intro\u001b[39m\u001b[34m(topic_keyword, rules, topic_description, dialogue_style)\u001b[39m\n\u001b[32m     24\u001b[39m gen_intro_contents = [topic_keyword, json.dumps(rules, ensure_ascii=\u001b[38;5;28;01mFalse\u001b[39;00m), topic_description, dialogue_style]\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# response 생성\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgemini-3-flash-preview\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen_intro_contents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msystem_instruction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmoderator_gen_intro_system_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_mime_type\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mapplication/json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_schema\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmoderator_gen_intro_object_schema\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response.text\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/Agent_EDU/lib/python3.11/site-packages/google/genai/models.py:5215\u001b[39m, in \u001b[36mModels.generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   5213\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m remaining_remote_calls_afc > \u001b[32m0\u001b[39m:\n\u001b[32m   5214\u001b[39m   i += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m5215\u001b[39m   response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5216\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparsed_config\u001b[49m\n\u001b[32m   5217\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5219\u001b[39m   function_map = _extra_utils.get_function_map(parsed_config)\n\u001b[32m   5220\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m function_map:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/Agent_EDU/lib/python3.11/site-packages/google/genai/models.py:3997\u001b[39m, in \u001b[36mModels._generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   3994\u001b[39m request_dict = _common.convert_to_dict(request_dict)\n\u001b[32m   3995\u001b[39m request_dict = _common.encode_unserializable_types(request_dict)\n\u001b[32m-> \u001b[39m\u001b[32m3997\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_api_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3998\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\n\u001b[32m   3999\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4001\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[32m   4002\u001b[39m     config, \u001b[33m'\u001b[39m\u001b[33mshould_return_http_response\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4003\u001b[39m ):\n\u001b[32m   4004\u001b[39m   return_value = types.GenerateContentResponse(sdk_http_response=response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/Agent_EDU/lib/python3.11/site-packages/google/genai/_api_client.py:1386\u001b[39m, in \u001b[36mBaseApiClient.request\u001b[39m\u001b[34m(self, http_method, path, request_dict, http_options)\u001b[39m\n\u001b[32m   1376\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrequest\u001b[39m(\n\u001b[32m   1377\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1378\u001b[39m     http_method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1381\u001b[39m     http_options: Optional[HttpOptionsOrDict] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1382\u001b[39m ) -> SdkHttpResponse:\n\u001b[32m   1383\u001b[39m   http_request = \u001b[38;5;28mself\u001b[39m._build_request(\n\u001b[32m   1384\u001b[39m       http_method, path, request_dict, http_options\n\u001b[32m   1385\u001b[39m   )\n\u001b[32m-> \u001b[39m\u001b[32m1386\u001b[39m   response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1387\u001b[39m   response_body = (\n\u001b[32m   1388\u001b[39m       response.response_stream[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m response.response_stream \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   1389\u001b[39m   )\n\u001b[32m   1390\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m SdkHttpResponse(headers=response.headers, body=response_body)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/Agent_EDU/lib/python3.11/site-packages/google/genai/_api_client.py:1222\u001b[39m, in \u001b[36mBaseApiClient._request\u001b[39m\u001b[34m(self, http_request, http_options, stream)\u001b[39m\n\u001b[32m   1219\u001b[39m     retry = tenacity.Retrying(**retry_kwargs)\n\u001b[32m   1220\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m retry(\u001b[38;5;28mself\u001b[39m._request_once, http_request, stream)  \u001b[38;5;66;03m# type: ignore[no-any-return]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1222\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_once\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/Agent_EDU/lib/python3.11/site-packages/tenacity/__init__.py:477\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    475\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/Agent_EDU/lib/python3.11/site-packages/tenacity/__init__.py:378\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    376\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/Agent_EDU/lib/python3.11/site-packages/tenacity/__init__.py:420\u001b[39m, in \u001b[36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    418\u001b[39m retry_exc = \u001b[38;5;28mself\u001b[39m.retry_error_cls(fut)\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reraise:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mretry_exc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfut\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexception\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/Agent_EDU/lib/python3.11/site-packages/tenacity/__init__.py:187\u001b[39m, in \u001b[36mRetryError.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreraise\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> t.NoReturn:\n\u001b[32m    186\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.last_attempt.failed:\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlast_attempt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/Agent_EDU/lib/python3.11/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/Agent_EDU/lib/python3.11/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/Agent_EDU/lib/python3.11/site-packages/tenacity/__init__.py:480\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m         result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    482\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/Agent_EDU/lib/python3.11/site-packages/google/genai/_api_client.py:1199\u001b[39m, in \u001b[36mBaseApiClient._request_once\u001b[39m\u001b[34m(self, http_request, stream)\u001b[39m\n\u001b[32m   1191\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1192\u001b[39m   response = \u001b[38;5;28mself\u001b[39m._httpx_client.request(\n\u001b[32m   1193\u001b[39m       method=http_request.method,\n\u001b[32m   1194\u001b[39m       url=http_request.url,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1197\u001b[39m       timeout=http_request.timeout,\n\u001b[32m   1198\u001b[39m   )\n\u001b[32m-> \u001b[39m\u001b[32m1199\u001b[39m   \u001b[43merrors\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAPIError\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1200\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[32m   1201\u001b[39m       response.headers, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response.text]\n\u001b[32m   1202\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/Agent_EDU/lib/python3.11/site-packages/google/genai/errors.py:121\u001b[39m, in \u001b[36mAPIError.raise_for_response\u001b[39m\u001b[34m(cls, response)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    119\u001b[39m   response_json = response.body_segments[\u001b[32m0\u001b[39m].get(\u001b[33m'\u001b[39m\u001b[33merror\u001b[39m\u001b[33m'\u001b[39m, {})\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraise_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/Agent_EDU/lib/python3.11/site-packages/google/genai/errors.py:146\u001b[39m, in \u001b[36mAPIError.raise_error\u001b[39m\u001b[34m(cls, status_code, response_json, response)\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Raises an appropriate APIError subclass based on the status code.\u001b[39;00m\n\u001b[32m    133\u001b[39m \n\u001b[32m    134\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    143\u001b[39m \u001b[33;03m  APIError: For other error status codes.\u001b[39;00m\n\u001b[32m    144\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[32m400\u001b[39m <= status_code < \u001b[32m500\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ClientError(status_code, response_json, response)\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[32m500\u001b[39m <= status_code < \u001b[32m600\u001b[39m:\n\u001b[32m    148\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ServerError(status_code, response_json, response)\n",
      "\u001b[31mClientError\u001b[39m: 403 PERMISSION_DENIED. {'error': {'code': 403, 'message': 'Your API key was reported as leaked. Please use another API key.', 'status': 'PERMISSION_DENIED'}}"
     ]
    }
   ],
   "source": [
    "from Gen_intro import llm_moderator_generate_intro\n",
    "\n",
    "# 가짜 입력값 JSON 데이터\n",
    "sample_contents = {\n",
    "    \"mode_configuration\": {\n",
    "        \"dialogue_style\": \"Debate_Mode\",\n",
    "        \"interaction_goal\": \"Concept_Verification\",\n",
    "        \"goal_description\": \"교과서적 개념에 대해 사용자가 설명하고, 상대 에이전트가 이를 반박하거나 허점을 지적하며 개념의 정확성을 검증함\"\n",
    "    },\n",
    "    \"content_context\": {\n",
    "        \"target_topic\": {\n",
    "            \"keyword\": \"DQN (Deep Q-Network)\",\n",
    "            \"description\": \"DQN의 주요 메커니즘인 Experience Replay와 Target Network의 필요성에 대한 개념적 방어\"\n",
    "        },\n",
    "        \"knowledge_boundary\": \"Lecture_Only\"\n",
    "    },\n",
    "    \"session_rules\": {\n",
    "        \"max_turns\": 5,\n",
    "        \"difficulty_parameter\": {\n",
    "            \"level\": \"High\",\n",
    "            \"custom_constraints\": [\n",
    "                \"논리적 비약이 있을 경우 즉시 지적할 것\",\n",
    "                \"수식보다는 개념의 인과관계 위주로 반박할 것\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# 함수 입력 파라미터로 변수 정의 (가짜 입력값)\n",
    "topic_keyword = sample_contents[\"content_context\"][\"target_topic\"][\"keyword\"]\n",
    "topic_description = sample_contents[\"content_context\"][\"target_topic\"][\"description\"]\n",
    "dialogue_style = sample_contents[\"mode_configuration\"][\"dialogue_style\"]\n",
    "rules = sample_contents[\"session_rules\"]\n",
    "\n",
    "response = llm_moderator_generate_intro(topic_keyword, rules, topic_description, dialogue_style)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81049fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\"message\": \"지금부터 DQN(Deep Q-Network)의 핵심 기법인 Experience Replay와 Target Network의 필요성에 대한 심화 비판 토론을 시작합니다. 본 세션은 난이도 '상'으로 설정되었으며 총 5턴 동안 진행됩니다. 이번 토론에서는 단순한 수식의 나열보다는 각 개념 사이의 인과관계를 중심으로 논리적 비약이 없는지 엄격하게 검증할 것입니다. 먼저, DQN에서 이 두 메커니즘이 제거되었을 때 발생하는 학습 불안정성의 근본적인 원인이 무엇인지 논리적으로 증명하며 입론을 시작해 주십시오.\",\n",
      "\"opening_strategy\": \"Academic defense framing for a high-difficulty session. It explicitly warns the user that logical causality will be prioritized over formulas, establishing a rigorous and critical atmosphere.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232f19ae",
   "metadata": {},
   "source": [
    "# 2) check Safety 생성 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8d45733",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Check_safety import llm_moderator_check_safety\n",
    "\n",
    "user_input = \"안녕?\"\n",
    "topic_keyword = \"DQN (Deep Q-Network)\"\n",
    "topic_description = \"DQN의 주요 메커니즘인 Experience Replay와 Target Network의 필요성에 대한 개념적 방어\"\n",
    "\n",
    "safety_check_str = llm_moderator_check_safety(user_input, topic_keyword, topic_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eaeb151d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"is_valid\":false,\"reason\":\"인삿말이나 잡담은 토론과 무관합니다. DQN의 Experience Replay와 Target Network의 필요성에 대한 구체적인 의견이나 논거를 제시해 주세요.\",\"violation_type\":\"Off_Topic\"}\n"
     ]
    }
   ],
   "source": [
    "print(safety_check_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d8ab11",
   "metadata": {},
   "source": [
    "# 3) Eval_assess_step 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c290abdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Eval_assess_step import llm_evaluator_assess_step\n",
    "#user_input = \"DQN은 Experience Replay를 사용하여 샘플 간의 상관관계를 줄이고, Target Network를 통해 학습의 안정성을 확보합니다. Experience Replay는 과거 경험을 버퍼에 저장했다가 무작위로 샘플링하여 학습하므로 연속된 샘플 간의 상관성을 제거할 수 있습니다.\"\n",
    "user_input = \"몰라 어케알아 그딴걸\"\n",
    "last_attack = \"DQN에서 Experience Replay가 정말 필요한가요? 그냥 최신 데이터만 사용하면 더 효율적이지 않을까요?\"\n",
    "\n",
    "lecture_material = \"/Users/jhkim/Desktop/Debate_Agent/phase2/lecture_materials/DQN_lecture.txt\"\n",
    "\n",
    "dialogue_style = \"Concept_Learning\"\n",
    "\n",
    "difficulty = \"Medium\"\n",
    "\n",
    "eval_result_str = llm_evaluator_assess_step(user_input, last_attack, lecture_material, dialogue_style, difficulty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8fb65d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"score_delta\":-8,\"rationale\":\"DQN의 핵심 개념인 Experience Replay의 필요성(샘플 간 상관관계 제거 및 안정적 학습)에 대해 전혀 설명하지 않고 답변을 회피했습니다. 중급 난이도에서 요구되는 최소한의 개념 설명이 이루어지지 않았습니다.\",\"detected_logic_flaw\":\"Refusal to Answer\"}\n"
     ]
    }
   ],
   "source": [
    "print(eval_result_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5428fd",
   "metadata": {},
   "source": [
    "# 4) Gen_attack 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "645fdb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Debater_Generate_Attack import llm_debater_generate_attack\n",
    "\n",
    "# 가짜 데이터 (테스트용)\n",
    "user_input = \"Experience Replay는 과거 경험을 재사용하지만, 타겟 네트워크를 주기적으로 업데이트함으로써 stale policy 문제를 완화하고, 샘플 간 상관관계를 제거하여 학습 안정성을 높입니다.\"\n",
    "\n",
    "conversation_history = [\n",
    "    # Turn 1 - AI의 첫 공격\n",
    "    {\n",
    "        \"role\": \"agent\",\n",
    "        \"content\": \"DQN의 Experience Replay가 샘플 효율성을 높인다고 주장하셨는데, 과거 경험을 재사용하는 것이 오히려 오래된 정책(stale policy)으로 학습하게 만들어 수렴을 방해하는 것 아닙니까?\",\n",
    "        \"meta\": {\"tactic\": \"Logic_Error_Point\"}\n",
    "    },\n",
    "]\n",
    "\n",
    "lecture_material = \"/Users/jhkim/Desktop/Debate_Agent/phase2/lecture_materials/DQN_lecture.txt\"\n",
    "\n",
    "dialogue_style = \"Concept_Learning\"\n",
    "\n",
    "difficulty = \"High\"\n",
    "\n",
    "response = llm_debater_generate_attack(user_input, conversation_history, lecture_material, difficulty, dialogue_style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b8d4a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"argument\": \"타겟 네트워크가 Experience Replay의 stale policy 문제를 완화한다고요? 오히려 타겟 네트워크는 학습의 안정성을 위해 의도적으로 'stale'한 파라미터를 고정하여 사용하는 기법입니다. 리플레이 버퍼에 쌓인 과거의 데이터가 현재 정책과 동떨어져 발생하는 본질적인 편향(bias) 문제를, 단순히 타겟 값을 고정하는 타겟 네트워크가 어떻게 직접적으로 '완화'할 수 있다는 건지 논리적 근거를 제시하십시오. 두 기법은 서로 다른 문제를 해결하기 위한 장치인데, 개념을 교묘하게 섞어서 얼버무리는 것 아닙니까?\",\n",
      "  \"attack_tactic\": \"Logic_Error_Point\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4386fd5",
   "metadata": {},
   "source": [
    "# 5) Eval_assess_closing 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "250f3878",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Eval_assess_closing import llm_evaluator_assess_closing\n",
    "\n",
    "final_user_speech = \"모르겠다, 니 다해라\"\n",
    "history = [\n",
    "    {\n",
    "        \"role\": \"agent\",\n",
    "        \"content\": \"DQN의 Experience Replay가 샘플 효율성을 높인다고 주장하셨는데, 과거 경험을 재사용하는 것이 오히려 오래된 정책(stale policy)으로 학습하게 만들어 수렴을 방해하는 것 아닙니까?\",\n",
    "        \"meta\": {\"tactic\": \"Logic_Error_Point\"}\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Experience Replay는 i.i.d 가정을 만족시키기 위해 필요합니다. 연속된 샘플 간의 correlation을 깨뜨려 학습의 안정성을 높입니다.\"\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"role\": \"agent\",\n",
    "        \"content\": \"i.i.d를 만족시킨다는 주장은 이해하겠습니다만, 그렇다면 왜 굳이 Target Network까지 필요한 겁니까? Experience Replay만으로도 충분하지 않나요? 두 메커니즘의 역할이 중복되는 것 아닙니까?\",\n",
    "        \"meta\": {\"tactic\": \"Deep_Inquiry\"}\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Target Network는 다른 목적입니다. Q-learning의 업데이트 과정에서 target 값이 계속 변하면 moving target 문제가 발생합니다. Target Network를 고정시켜서...\"\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"role\": \"agent\",\n",
    "        \"content\": \"그 설명은 피상적입니다. Moving target이 문제라면 learning rate를 낮추는 것으로도 해결 가능하지 않습니까? Target Network의 고유한 필요성을 수식적으로 증명하지 못하고 계십니다.\",\n",
    "        \"meta\": {\"tactic\": \"Sarcastic_Refutation\"}\n",
    "    }\n",
    "]\n",
    "lecture_material = \"/Users/jhkim/Desktop/Debate_Agent/phase2/lecture_materials/DQN_lecture.txt\"\n",
    "dialogue_style = \"Concept_Learning\"\n",
    "difficulty = \"High\"\n",
    "\n",
    "response = llm_evaluator_assess_closing(final_user_speech, history, lecture_material, dialogue_style, difficulty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1985e76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"score_delta\": -10, \"final_impression\": \"학습 중인 핵심 개념에 대한 논리적 방어를 포기하고 대화를 중단하였으므로, 비판적 탐구와 개념 이해의 요건을 모두 충족하지 못했습니다.\"}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4173726",
   "metadata": {},
   "source": [
    "# 6) Generate_Summary 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ada97a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Generate_Summary import llm_moderator_generate_summary\n",
    "\n",
    "# contents 생성\n",
    "history = [\n",
    "    {\n",
    "        \"role\": \"agent\",\n",
    "        \"content\": \"DQN에서 Experience Replay를 사용하면 샘플 효율성이 높아진다고 하셨는데, 오래된 경험을 재사용하는 것이 오히려 현재 정책과 맞지 않아 학습을 방해하는 것 아닙니까?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Experience Replay는 i.i.d 가정을 만족시키기 위해 필요합니다. 연속된 샘플 간의 correlation을 제거합니다.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"agent\",\n",
    "        \"content\": \"그렇다면 Target Network는 왜 필요합니까? Replay Buffer만으로 충분하지 않나요?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Target Network는 moving target 문제를 해결합니다. Q-learning에서 target 값이 계속 변하면 학습이 불안정해집니다.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"agent\",\n",
    "        \"content\": \"Moving target 문제라고요? Learning rate를 낮추면 되지 않습니까?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Learning rate만으로는 부족합니다. Bellman update에서 target이 고정되어야 수렴이 보장됩니다.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"agent\",\n",
    "        \"content\": \"Overestimation bias는 어떻게 해결하셨나요?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"그건... DQN의 한계입니다. Double DQN에서 해결되었습니다.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"[최후 변론] DQN은 Experience Replay와 Target Network를 통해 deep learning을 강화학습에 안정적으로 결합한 혁명적인 방법입니다.\"\n",
    "    }\n",
    "]\n",
    "evaluation_logs = [\n",
    "    {\n",
    "        \"turn\": 1,\n",
    "        \"score_now\": 55,\n",
    "        \"reason\": \"i.i.d 가정과 correlation 제거를 정확히 언급함.\"\n",
    "    },\n",
    "    {\n",
    "        \"turn\": 2,\n",
    "        \"score_now\": 63,\n",
    "        \"reason\": \"Target Network의 역할을 명확히 구분하고 moving target 문제를 지적함.\"\n",
    "    },\n",
    "    {\n",
    "        \"turn\": 3,\n",
    "        \"score_now\": 66,\n",
    "        \"reason\": \"Bellman equation을 언급했으나 수식적 증명까지는 도달하지 못함.\"\n",
    "    },\n",
    "    {\n",
    "        \"turn\": 4,\n",
    "        \"score_now\": 64,\n",
    "        \"reason\": \"Overestimation bias를 제대로 방어하지 못하고 한계라고만 인정함.\"\n",
    "    },\n",
    "    {\n",
    "        \"turn\": \"Final\",\n",
    "        \"score_now\": 70,\n",
    "        \"reason\": \"DQN의 역사적 의의를 강조한 설득력 있는 최후 변론.\"\n",
    "    }\n",
    "]\n",
    "final_status = \"WIN\"\n",
    "\n",
    "response = llm_moderator_generate_summary(history, evaluation_logs, final_status)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "531c306d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"summary_text\": \"이번 토론은 DQN의 핵심 아키텍처를 주제로 진행되었으며, 사용자님은 기술적 개념에 대한 정확한 이해를 바탕으로 승리(WIN)를 거두셨습니다. 특히 Experience Replay의 필요성을 'i.i.d 가정 만족'과 '샘플 간 상관관계(correlation) 제거'로 명확히 정의하며 논리적 우위를 점하신 점이 훌륭했습니다. 에이전트가 제기한 Target Network의 불필요성 논란에 대해서도 'moving target 문제'와 'Bellman update의 수렴성'을 근거로 논리적으로 방어해 내셨습니다. 비록 'Overestimation bias' 질문에서 구체적인 수식적 원인 분석보다는 'Double DQN'이라는 대안 제시와 한계 인정에 그쳤으나, 전체적인 흐름을 유지하는 데는 충분했습니다. 마지막 변론에서 DQN이 딥러닝과 강화학습을 결합한 '혁명적인 방법'임을 강조하며 토론의 가치를 높인 점이 승리의 결정적 요인이었습니다.\",\n",
      "  \"key_takeaways\": [\n",
      "    \"i.i.d (Independent and Identically Distributed)\",\n",
      "    \"Moving Target Problem\",\n",
      "    \"Overestimation Bias\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Agent_EDU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
