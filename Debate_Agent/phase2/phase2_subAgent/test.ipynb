{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b26d28ca",
   "metadata": {},
   "source": [
    "# 1) intro 생성 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21ddaea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Gen_intro import llm_moderator_generate_intro\n",
    "\n",
    "# 가짜 입력값 JSON 데이터\n",
    "sample_contents = {\n",
    "    \"mode_configuration\": {\n",
    "        \"dialogue_style\": \"Debate_Mode\",\n",
    "        \"interaction_goal\": \"Concept_Verification\",\n",
    "        \"goal_description\": \"교과서적 개념에 대해 사용자가 설명하고, 상대 에이전트가 이를 반박하거나 허점을 지적하며 개념의 정확성을 검증함\"\n",
    "    },\n",
    "    \"content_context\": {\n",
    "        \"target_topic\": {\n",
    "            \"keyword\": \"DQN (Deep Q-Network)\",\n",
    "            \"description\": \"DQN의 주요 메커니즘인 Experience Replay와 Target Network의 필요성에 대한 개념적 방어\"\n",
    "        },\n",
    "        \"knowledge_boundary\": \"Lecture_Only\"\n",
    "    },\n",
    "    \"session_rules\": {\n",
    "        \"max_turns\": 5,\n",
    "        \"difficulty_parameter\": {\n",
    "            \"level\": \"High\",\n",
    "            \"custom_constraints\": [\n",
    "                \"논리적 비약이 있을 경우 즉시 지적할 것\",\n",
    "                \"수식보다는 개념의 인과관계 위주로 반박할 것\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# 함수 입력 파라미터로 변수 정의 (가짜 입력값)\n",
    "topic_keyword = sample_contents[\"content_context\"][\"target_topic\"][\"keyword\"]\n",
    "topic_description = sample_contents[\"content_context\"][\"target_topic\"][\"description\"]\n",
    "dialogue_style = sample_contents[\"mode_configuration\"][\"dialogue_style\"]\n",
    "rules = sample_contents[\"session_rules\"]\n",
    "\n",
    "response = llm_moderator_generate_intro(topic_keyword, rules, topic_description, dialogue_style)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81049fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\"message\": \"지금부터 DQN(Deep Q-Network)의 핵심 기법인 Experience Replay와 Target Network의 필요성에 대한 심화 비판 토론을 시작합니다. 본 세션은 난이도 '상'으로 설정되었으며 총 5턴 동안 진행됩니다. 이번 토론에서는 단순한 수식의 나열보다는 각 개념 사이의 인과관계를 중심으로 논리적 비약이 없는지 엄격하게 검증할 것입니다. 먼저, DQN에서 이 두 메커니즘이 제거되었을 때 발생하는 학습 불안정성의 근본적인 원인이 무엇인지 논리적으로 증명하며 입론을 시작해 주십시오.\",\n",
      "\"opening_strategy\": \"Academic defense framing for a high-difficulty session. It explicitly warns the user that logical causality will be prioritized over formulas, establishing a rigorous and critical atmosphere.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232f19ae",
   "metadata": {},
   "source": [
    "# 2) check Safety 생성 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8d45733",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Check_safety import llm_moderator_check_safety\n",
    "\n",
    "user_input = \"안녕?\"\n",
    "topic_keyword = \"DQN (Deep Q-Network)\"\n",
    "topic_description = \"DQN의 주요 메커니즘인 Experience Replay와 Target Network의 필요성에 대한 개념적 방어\"\n",
    "\n",
    "safety_check_str = llm_moderator_check_safety(user_input, topic_keyword, topic_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eaeb151d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"is_valid\":false,\"reason\":\"인삿말이나 잡담은 토론과 무관합니다. DQN의 Experience Replay와 Target Network의 필요성에 대한 구체적인 의견이나 논거를 제시해 주세요.\",\"violation_type\":\"Off_Topic\"}\n"
     ]
    }
   ],
   "source": [
    "print(safety_check_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d8ab11",
   "metadata": {},
   "source": [
    "# 3) Eval_assess_step 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c290abdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Eval_assess_step import llm_evaluator_assess_step\n",
    "#user_input = \"DQN은 Experience Replay를 사용하여 샘플 간의 상관관계를 줄이고, Target Network를 통해 학습의 안정성을 확보합니다. Experience Replay는 과거 경험을 버퍼에 저장했다가 무작위로 샘플링하여 학습하므로 연속된 샘플 간의 상관성을 제거할 수 있습니다.\"\n",
    "user_input = \"몰라 어케알아 그딴걸\"\n",
    "last_attack = \"DQN에서 Experience Replay가 정말 필요한가요? 그냥 최신 데이터만 사용하면 더 효율적이지 않을까요?\"\n",
    "\n",
    "lecture_material = \"/Users/jhkim/Desktop/Debate_Agent/phase2/lecture_materials/DQN_lecture.txt\"\n",
    "\n",
    "dialogue_style = \"Concept_Learning\"\n",
    "\n",
    "difficulty = \"Medium\"\n",
    "\n",
    "eval_result_str = llm_evaluator_assess_step(user_input, last_attack, lecture_material, dialogue_style, difficulty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8fb65d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"score_delta\":-8,\"rationale\":\"DQN의 핵심 개념인 Experience Replay의 필요성(샘플 간 상관관계 제거 및 안정적 학습)에 대해 전혀 설명하지 않고 답변을 회피했습니다. 중급 난이도에서 요구되는 최소한의 개념 설명이 이루어지지 않았습니다.\",\"detected_logic_flaw\":\"Refusal to Answer\"}\n"
     ]
    }
   ],
   "source": [
    "print(eval_result_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5428fd",
   "metadata": {},
   "source": [
    "# 4) Gen_attack 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "645fdb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Debater_Generate_Attack import llm_debater_generate_attack\n",
    "\n",
    "# 가짜 데이터 (테스트용)\n",
    "user_input = \"Experience Replay는 과거 경험을 재사용하지만, 타겟 네트워크를 주기적으로 업데이트함으로써 stale policy 문제를 완화하고, 샘플 간 상관관계를 제거하여 학습 안정성을 높입니다.\"\n",
    "\n",
    "conversation_history = [\n",
    "    # Turn 1 - AI의 첫 공격\n",
    "    {\n",
    "        \"role\": \"agent\",\n",
    "        \"content\": \"DQN의 Experience Replay가 샘플 효율성을 높인다고 주장하셨는데, 과거 경험을 재사용하는 것이 오히려 오래된 정책(stale policy)으로 학습하게 만들어 수렴을 방해하는 것 아닙니까?\",\n",
    "        \"meta\": {\"tactic\": \"Logic_Error_Point\"}\n",
    "    },\n",
    "]\n",
    "\n",
    "lecture_material = \"/Users/jhkim/Desktop/Debate_Agent/phase2/lecture_materials/DQN_lecture.txt\"\n",
    "\n",
    "dialogue_style = \"Concept_Learning\"\n",
    "\n",
    "difficulty = \"High\"\n",
    "\n",
    "response = llm_debater_generate_attack(user_input, conversation_history, lecture_material, difficulty, dialogue_style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b8d4a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"argument\": \"타겟 네트워크가 Experience Replay의 stale policy 문제를 완화한다고요? 오히려 타겟 네트워크는 학습의 안정성을 위해 의도적으로 'stale'한 파라미터를 고정하여 사용하는 기법입니다. 리플레이 버퍼에 쌓인 과거의 데이터가 현재 정책과 동떨어져 발생하는 본질적인 편향(bias) 문제를, 단순히 타겟 값을 고정하는 타겟 네트워크가 어떻게 직접적으로 '완화'할 수 있다는 건지 논리적 근거를 제시하십시오. 두 기법은 서로 다른 문제를 해결하기 위한 장치인데, 개념을 교묘하게 섞어서 얼버무리는 것 아닙니까?\",\n",
      "  \"attack_tactic\": \"Logic_Error_Point\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4386fd5",
   "metadata": {},
   "source": [
    "# 5) Eval_assess_closing 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "250f3878",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Eval_assess_closing import llm_evaluator_assess_closing\n",
    "\n",
    "final_user_speech = \"모르겠다, 니 다해라\"\n",
    "history = [\n",
    "    {\n",
    "        \"role\": \"agent\",\n",
    "        \"content\": \"DQN의 Experience Replay가 샘플 효율성을 높인다고 주장하셨는데, 과거 경험을 재사용하는 것이 오히려 오래된 정책(stale policy)으로 학습하게 만들어 수렴을 방해하는 것 아닙니까?\",\n",
    "        \"meta\": {\"tactic\": \"Logic_Error_Point\"}\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Experience Replay는 i.i.d 가정을 만족시키기 위해 필요합니다. 연속된 샘플 간의 correlation을 깨뜨려 학습의 안정성을 높입니다.\"\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"role\": \"agent\",\n",
    "        \"content\": \"i.i.d를 만족시킨다는 주장은 이해하겠습니다만, 그렇다면 왜 굳이 Target Network까지 필요한 겁니까? Experience Replay만으로도 충분하지 않나요? 두 메커니즘의 역할이 중복되는 것 아닙니까?\",\n",
    "        \"meta\": {\"tactic\": \"Deep_Inquiry\"}\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Target Network는 다른 목적입니다. Q-learning의 업데이트 과정에서 target 값이 계속 변하면 moving target 문제가 발생합니다. Target Network를 고정시켜서...\"\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"role\": \"agent\",\n",
    "        \"content\": \"그 설명은 피상적입니다. Moving target이 문제라면 learning rate를 낮추는 것으로도 해결 가능하지 않습니까? Target Network의 고유한 필요성을 수식적으로 증명하지 못하고 계십니다.\",\n",
    "        \"meta\": {\"tactic\": \"Sarcastic_Refutation\"}\n",
    "    }\n",
    "]\n",
    "lecture_material = \"/Users/jhkim/Desktop/Debate_Agent/phase2/lecture_materials/DQN_lecture.txt\"\n",
    "dialogue_style = \"Concept_Learning\"\n",
    "difficulty = \"High\"\n",
    "\n",
    "response = llm_evaluator_assess_closing(final_user_speech, history, lecture_material, dialogue_style, difficulty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1985e76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"score_delta\": -10, \"final_impression\": \"학습 중인 핵심 개념에 대한 논리적 방어를 포기하고 대화를 중단하였으므로, 비판적 탐구와 개념 이해의 요건을 모두 충족하지 못했습니다.\"}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4173726",
   "metadata": {},
   "source": [
    "# 6) Generate_Summary 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ada97a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Generate_Summary import llm_moderator_generate_summary\n",
    "\n",
    "# contents 생성\n",
    "history = [\n",
    "    {\n",
    "        \"role\": \"agent\",\n",
    "        \"content\": \"DQN에서 Experience Replay를 사용하면 샘플 효율성이 높아진다고 하셨는데, 오래된 경험을 재사용하는 것이 오히려 현재 정책과 맞지 않아 학습을 방해하는 것 아닙니까?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Experience Replay는 i.i.d 가정을 만족시키기 위해 필요합니다. 연속된 샘플 간의 correlation을 제거합니다.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"agent\",\n",
    "        \"content\": \"그렇다면 Target Network는 왜 필요합니까? Replay Buffer만으로 충분하지 않나요?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Target Network는 moving target 문제를 해결합니다. Q-learning에서 target 값이 계속 변하면 학습이 불안정해집니다.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"agent\",\n",
    "        \"content\": \"Moving target 문제라고요? Learning rate를 낮추면 되지 않습니까?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Learning rate만으로는 부족합니다. Bellman update에서 target이 고정되어야 수렴이 보장됩니다.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"agent\",\n",
    "        \"content\": \"Overestimation bias는 어떻게 해결하셨나요?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"그건... DQN의 한계입니다. Double DQN에서 해결되었습니다.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"[최후 변론] DQN은 Experience Replay와 Target Network를 통해 deep learning을 강화학습에 안정적으로 결합한 혁명적인 방법입니다.\"\n",
    "    }\n",
    "]\n",
    "evaluation_logs = [\n",
    "    {\n",
    "        \"turn\": 1,\n",
    "        \"score_now\": 55,\n",
    "        \"reason\": \"i.i.d 가정과 correlation 제거를 정확히 언급함.\"\n",
    "    },\n",
    "    {\n",
    "        \"turn\": 2,\n",
    "        \"score_now\": 63,\n",
    "        \"reason\": \"Target Network의 역할을 명확히 구분하고 moving target 문제를 지적함.\"\n",
    "    },\n",
    "    {\n",
    "        \"turn\": 3,\n",
    "        \"score_now\": 66,\n",
    "        \"reason\": \"Bellman equation을 언급했으나 수식적 증명까지는 도달하지 못함.\"\n",
    "    },\n",
    "    {\n",
    "        \"turn\": 4,\n",
    "        \"score_now\": 64,\n",
    "        \"reason\": \"Overestimation bias를 제대로 방어하지 못하고 한계라고만 인정함.\"\n",
    "    },\n",
    "    {\n",
    "        \"turn\": \"Final\",\n",
    "        \"score_now\": 70,\n",
    "        \"reason\": \"DQN의 역사적 의의를 강조한 설득력 있는 최후 변론.\"\n",
    "    }\n",
    "]\n",
    "final_status = \"WIN\"\n",
    "\n",
    "response = llm_moderator_generate_summary(history, evaluation_logs, final_status)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "531c306d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"summary_text\": \"이번 토론은 DQN의 핵심 아키텍처를 주제로 진행되었으며, 사용자님은 기술적 개념에 대한 정확한 이해를 바탕으로 승리(WIN)를 거두셨습니다. 특히 Experience Replay의 필요성을 'i.i.d 가정 만족'과 '샘플 간 상관관계(correlation) 제거'로 명확히 정의하며 논리적 우위를 점하신 점이 훌륭했습니다. 에이전트가 제기한 Target Network의 불필요성 논란에 대해서도 'moving target 문제'와 'Bellman update의 수렴성'을 근거로 논리적으로 방어해 내셨습니다. 비록 'Overestimation bias' 질문에서 구체적인 수식적 원인 분석보다는 'Double DQN'이라는 대안 제시와 한계 인정에 그쳤으나, 전체적인 흐름을 유지하는 데는 충분했습니다. 마지막 변론에서 DQN이 딥러닝과 강화학습을 결합한 '혁명적인 방법'임을 강조하며 토론의 가치를 높인 점이 승리의 결정적 요인이었습니다.\",\n",
      "  \"key_takeaways\": [\n",
      "    \"i.i.d (Independent and Identically Distributed)\",\n",
      "    \"Moving Target Problem\",\n",
      "    \"Overestimation Bias\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Agent_EDU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
