{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5ac3068",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "import pathlib\n",
    "import httpx\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f4a871c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 환경 변수 불러오기\n",
    "api_key = os.environ.get('MY_API_KEY')\n",
    "# client 설정\n",
    "client = genai.Client(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9d9853f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 미리 정의된 스키마, 시스템 프롬포트 불러오기 (서버 제작시 수정 필요)\n",
    "moderator_gen_summary_object_schema_pwd = \"/Users/jhkim/Desktop/Debate_Agent/phase2/phase2_subAgent/phase2_subAgent_Prompt/moderator_gen_summary_Prompt/moderator_gen_summary_object.json\"\n",
    "moderator_gen_summary_system_prompt_pwd = \"/Users/jhkim/Desktop/Debate_Agent/phase2/phase2_subAgent/phase2_subAgent_Prompt/moderator_gen_summary_Prompt/moderator_gen_summary_SystemPrompt.md\"\n",
    "\n",
    "with open(moderator_gen_summary_object_schema_pwd, 'r', encoding='utf-8') as f:\n",
    "    moderator_gen_summary_object_schema = json.load(f)\n",
    "\n",
    "with open(moderator_gen_summary_system_prompt_pwd, 'r', encoding='utf-8') as f:\n",
    "    moderator_gen_summary_system_prompt = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1849d648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contents 생성\n",
    "history = [\n",
    "    {\n",
    "        \"role\": \"agent\",\n",
    "        \"content\": \"DQN에서 Experience Replay를 사용하면 샘플 효율성이 높아진다고 하셨는데, 오래된 경험을 재사용하는 것이 오히려 현재 정책과 맞지 않아 학습을 방해하는 것 아닙니까?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Experience Replay는 i.i.d 가정을 만족시키기 위해 필요합니다. 연속된 샘플 간의 correlation을 제거합니다.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"agent\",\n",
    "        \"content\": \"그렇다면 Target Network는 왜 필요합니까? Replay Buffer만으로 충분하지 않나요?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Target Network는 moving target 문제를 해결합니다. Q-learning에서 target 값이 계속 변하면 학습이 불안정해집니다.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"agent\",\n",
    "        \"content\": \"Moving target 문제라고요? Learning rate를 낮추면 되지 않습니까?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Learning rate만으로는 부족합니다. Bellman update에서 target이 고정되어야 수렴이 보장됩니다.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"agent\",\n",
    "        \"content\": \"Overestimation bias는 어떻게 해결하셨나요?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"그건... DQN의 한계입니다. Double DQN에서 해결되었습니다.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"[최후 변론] DQN은 Experience Replay와 Target Network를 통해 deep learning을 강화학습에 안정적으로 결합한 혁명적인 방법입니다.\"\n",
    "    }\n",
    "]\n",
    "evaluation_logs = [\n",
    "    {\n",
    "        \"turn\": 1,\n",
    "        \"score_now\": 55,\n",
    "        \"reason\": \"i.i.d 가정과 correlation 제거를 정확히 언급함.\"\n",
    "    },\n",
    "    {\n",
    "        \"turn\": 2,\n",
    "        \"score_now\": 63,\n",
    "        \"reason\": \"Target Network의 역할을 명확히 구분하고 moving target 문제를 지적함.\"\n",
    "    },\n",
    "    {\n",
    "        \"turn\": 3,\n",
    "        \"score_now\": 66,\n",
    "        \"reason\": \"Bellman equation을 언급했으나 수식적 증명까지는 도달하지 못함.\"\n",
    "    },\n",
    "    {\n",
    "        \"turn\": 4,\n",
    "        \"score_now\": 64,\n",
    "        \"reason\": \"Overestimation bias를 제대로 방어하지 못하고 한계라고만 인정함.\"\n",
    "    },\n",
    "    {\n",
    "        \"turn\": \"Final\",\n",
    "        \"score_now\": 70,\n",
    "        \"reason\": \"DQN의 역사적 의의를 강조한 설득력 있는 최후 변론.\"\n",
    "    }\n",
    "]\n",
    "final_status = \"WIN\"\n",
    "\n",
    "gen_summary_contents = [\n",
    "    \"history: \" + json.dumps(history),\n",
    "    \"evaluation_logs: \" + json.dumps(evaluation_logs),\n",
    "    \"final_status: \" + final_status\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07e288cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response 생성\n",
    "response = client.models.generate_content(\n",
    "    model = \"gemini-3-flash-preview\",\n",
    "    contents = gen_summary_contents,\n",
    "    config={\n",
    "        \"system_instruction\": moderator_gen_summary_system_prompt, \n",
    "        \"response_mime_type\": \"application/json\",\n",
    "        \"response_schema\": moderator_gen_summary_object_schema\n",
    "    })\n",
    "\n",
    "fianl_response = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d54a56f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"summary_text\":\"이번 토론은 강화학습의 혁신적 알고리즘인 DQN의 핵심 메커니즘을 주제로 진행되었으며, 사용자님은 최종 승리(WIN)를 거두셨습니다. 특히 Experience Replay가 'i.i.d 가정을 만족'시키고 '샘플 간의 Correlation을 제거'한다는 점을 정확히 짚어내어 논리적 우위를 점하셨습니다. 또한, Target Network의 필요성에 대해 'Moving Target 문제'를 언급하며 'Bellman update에서 Target이 고정되어야 수렴이 보장된다'는 핵심 원리를 설득력 있게 설명하신 점이 인상적이었습니다. 상대방의 Overestimation bias 공격에 대해서도 DQN의 한계를 인정하고 'Double DQN'이라는 대안을 제시함으로써 전문성을 보여주셨습니다. 다만, 벨만 방정식의 수식적 근거를 보강한다면 더욱 완벽한 논리가 될 것입니다.\",\"key_takeaways\":[\"i.i.d. (Independent and Identically Distributed)\",\"Moving Target Problem\",\"Overestimation Bias\"]}\n"
     ]
    }
   ],
   "source": [
    "print(fianl_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Agent_EDU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
